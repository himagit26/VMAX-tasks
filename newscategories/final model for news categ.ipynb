{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c6b2e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\HEEMA\n",
      "[nltk_data]     SAMEERA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\HEEMA\n",
      "[nltk_data]     SAMEERA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\HEEMA\n",
      "[nltk_data]     SAMEERA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fecb2443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Best Params - {'C': 10, 'solver': 'liblinear'}, Best Score - 0.7849\n",
      "Decision Tree: Best Params - {'max_depth': None, 'min_samples_split': 5}, Best Score - 0.7316\n",
      "Random Forest: Best Params - {'max_depth': None, 'n_estimators': 200}, Best Score - 0.8600\n",
      "SVM: Best Params - {'C': 10, 'kernel': 'linear'}, Best Score - 0.7560\n",
      "k-NN: Best Params - {'n_neighbors': 5, 'weights': 'distance'}, Best Score - 0.7091\n",
      "Naive Bayes: Best Params - {}, Best Score - 0.6457\n",
      "Gradient Boosting: Best Params - {'learning_rate': 0.5, 'n_estimators': 200}, Best Score - 0.8832\n",
      "FINAL BEST MODEL DETAILS\n",
      "\n",
      "Best model:Gradient Boosting Test Accuracy - 0.8864\n",
      "Predicted Label: COMMUNAL/RELIGIOUS\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load data\n",
    "data = pd.read_excel(r\"C:\\Users\\HEEMA SAMEERA\\OneDrive\\Desktop\\articlescateg.xlsx\")\n",
    "\n",
    "# Preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation and special characters\n",
    "    tokens = text.split()  # Tokenization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Remove stop words and lemmatize\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['text'] = data['text'].apply(preprocess)\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_features = vectorizer.fit_transform(data['text'])\n",
    "\n",
    "# Sentiment analysis using NLTK VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    sentiment_scores = analyzer.polarity_scores(text)\n",
    "    return sentiment_scores\n",
    "\n",
    "# Apply sentiment analysis\n",
    "data['sentiment'] = data['text'].apply(sentiment_analysis)\n",
    "\n",
    "# Convert sentiment scores to separate columns\n",
    "data = data.join(pd.json_normalize(data['sentiment']))\n",
    "\n",
    "# Drop the original 'sentiment' column\n",
    "data = data.drop(columns=['sentiment'])\n",
    "\n",
    "# Combine TF-IDF features with sentiment features\n",
    "sentiment_features = data[['neg', 'neu', 'pos', 'compound']].values\n",
    "X = np.hstack((tfidf_features.toarray(), sentiment_features))\n",
    "\n",
    "y = data['label']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'k-NN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),  # Using Gaussian Naive Bayes\n",
    "    'Gradient Boosting': GradientBoostingClassifier(),\n",
    "}\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {'C': [0.1, 1, 10], 'solver': ['liblinear', 'lbfgs']},\n",
    "    'Decision Tree': {'max_depth': [None, 10, 20, 30], 'min_samples_split': [2, 5, 10]},\n",
    "    'Random Forest': {'n_estimators': [100, 200], 'max_depth': [None, 10, 20]},\n",
    "    'SVM': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']},\n",
    "    'k-NN': {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']},\n",
    "    'Naive Bayes': {},  # Naive Bayes often doesn't require hyperparameter tuning\n",
    "    'Gradient Boosting': {'n_estimators': [100, 200], 'learning_rate': [0.01, 0.1, 0.5]},\n",
    "}\n",
    "\n",
    "# Set up cross-validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Grid Search for each model\n",
    "best_models = {}\n",
    "best_scores = {}\n",
    "for name, model in models.items():\n",
    "    grid_search = GridSearchCV(model, param_grids[name], cv=stratified_kfold, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[name] = grid_search.best_estimator_\n",
    "    best_scores[name] = grid_search.best_score_\n",
    "    print(f\"{name}: Best Params - {grid_search.best_params_}, Best Score - {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"FINAL BEST MODEL DETAILS\\n\")\n",
    "    \n",
    "# Select the best model based on cross-validation score\n",
    "best_model_name = max(best_scores, key=best_scores.get)\n",
    "best_model = best_models[best_model_name]\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Best model:{best_model_name} Test Accuracy - {accuracy:.4f}\")\n",
    "\n",
    "# Sample text to test\n",
    "sample_text = \"Religion is a basic good for all human beings everywhere, therefore religious freedom is a universal human right. It is neither unfair nor parochial, but a requirement of justice.\"\n",
    "# Preprocess the text\n",
    "preprocessed_text = preprocess(sample_text)\n",
    "\n",
    "# Vectorize the preprocessed text using the same vectorizer used for training\n",
    "text_vector = vectorizer.transform([preprocessed_text])\n",
    "\n",
    "# Make sure the number of features matches the training data\n",
    "if text_vector.shape[1] != X_train.shape[1]:\n",
    "    missing_features = X_train.shape[1] - text_vector.shape[1]\n",
    "    text_vector = np.pad(text_vector.toarray(), ((0, 0), (0, missing_features)), mode='constant')\n",
    "\n",
    "# Predict the label of the sample text using the best model\n",
    "predicted_label = best_model.predict(text_vector)[0]\n",
    "print(\"Predicted Label:\", predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f4a1533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: CYBER CRIME\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"cyber crimes all around\"\n",
    "# Preprocess the text\n",
    "preprocessed_text = preprocess(sample_text)\n",
    "\n",
    "# Vectorize the preprocessed text using the same vectorizer used for training\n",
    "text_vector = vectorizer.transform([preprocessed_text])\n",
    "\n",
    "# Make sure the number of features matches the training data\n",
    "if text_vector.shape[1] != X_train.shape[1]:\n",
    "    # Pad the text_vector with zeros for missing features\n",
    "    missing_features = X_train.shape[1] - text_vector.shape[1]\n",
    "    text_vector = np.pad(text_vector.toarray(), ((0, 0), (0, missing_features)), mode='constant')\n",
    "\n",
    "# Predict the label of the sample text using the best model\n",
    "predicted_label = best_model.predict(text_vector)[0]\n",
    "print(\"Predicted Label:\", predicted_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
